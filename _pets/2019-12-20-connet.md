---
title: "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling"
read_time: false
author_profile: false
tags:
  - Natural Language Processing
  - Crowd Annotaions
  - Unsupervised Cross-Domain Adaptation
---

Ouyu Lan∗, Xiao Huang∗, Bill Yuchen Lin, He Jiang, Liyuan Liu, Xiang Ren, *ACL 2020*.

[Paper](https://arxiv.org/abs/1910.04289){: .btn .btn--inverse}
[Code](https://github.com/INK-USC/ConNet){: .btn .btn--inverse}
[Cite](#to-cite-us){: .btn .btn--inverse}

## Introduction
Sequence labeling is a fundamental framework for various natural language processing problems. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios. In many cases, obtaining ground truth labels is costly, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (CONNET) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data...). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders.

## Method: Consensus Network
Our intuition mainly comes from the phenomenon that different sources of supervision have different strengths. To better model this nature, we need to (1) explicitly model the unique traits of different sources when training and (2) find best suitable sources for generalizing the learned model on unseen sentences. In this paper, we propose a novel framework, named Consensus Network (CONNET), for sequence labeling with multi-source supervisions. We represent the annotation patterns as different biases of annotators over a shared behavior pattern. Both annotator-invariant patterns and annotator-specific biases are modeled in a decoupled way. The first term comes through sharing part of low-level model parameters in a multi-task learning schema. For learning the biases, we decouple them from the model as the transformations on top-level tagging model parameters, such that they can capture the unique strength of each annotator. With such decoupled source representations, we further learn an attention network for dynamically assigning the best sources for every unseen sentence through composing a transformation that represents the agreement among sources (consensus).

## Experiments
Extensive experimental results in two scenarios show that our model outperforms strong baseline methods, on various tasks and with different encoders.

## To cite us
```markdown
@inproceedings{
    lan2020learning,
    title={Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling},
    author={Ouyu Lan and Xiao Huang and Bill Yuchen Lin and He Jiang and Liyuan Liu and Xiang Ren},
    booktitle={Proc. of ACL},
    year={2020},
    url={https://arxiv.org/abs/1910.04289}
    }
```

## Contact
If you have any questions about the paper or the code, please feel free to contact Ouyu Lan (lanouyu at gmail com) or Xiao Huang (huan183 at usc edu).